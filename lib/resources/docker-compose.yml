services:
  # LLMサーバー: Llama.cpp Server
  llama-server:
    image: ghcr.io/ggml-org/llama.cpp:server
    container_name: llama-server
    restart: always
    environment:
      - LLAMA_ARG_HF_REPO=Qwen/Qwen2.5-Coder-3B-Instruct-GGUF
      - LLAMA_ARG_HOST=0.0.0.0
      - LLAMA_ARG_PORT=8080
    networks:
      - backend-net

  # アプリサーバー: OpenWebUI
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    restart: always
    environment:
      # Llama-serverへの接続設定 (Docker内通信)
      - OPENAI_API_BASE_URL=http://llama-server:8080/v1
      - OPENAI_API_KEY=dummy  # ローカルLLMなのでダミー
      - WEBUI_AUTH=True
      - DEFAULT_USER_ROLE=user
      - ENABLE_SIGNUP_PASSWORD_CONFIRMATION=True
    volumes:
      - openwebui:/app/backend/data
    depends_on:
      - llama-server
      - fluent-bit
    logging:
      driver: fluentd
      options:
        fluentd-address: "localhost:24224"
        fluentd-async-connect: "true"
    networks:
      - frontend-net
      - backend-net
      - logging-net

  # リバースプロキシ: Nginx
  nginx:
    image: nginx:latest
    container_name: nginx-proxy
    restart: always
    ports:
      - "443:443"
    volumes:
      - ./nginx/conf.d:/etc/nginx/conf.d:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
    depends_on:
      - open-webui
    networks:
      - frontend-net
  
  # ログ転送: Fluentbit
  fluent-bit:
    image: fluent/fluent-bit:latest
    container_name: fluent-bit
    restart: always
    ports:
      - "24224:24224"
    environment:
      - DD_API_KEY=${DD_API_KEY}
    volumes:
      - ./fluent-bit.conf:/fluent-bit/etc/fluent-bit.conf
    networks:
      - logging-net

volumes:
  openwebui:

networks:
  frontend-net:
    driver: bridge
  backend-net:
    driver: bridge
  logging-net:
    driver: bridge